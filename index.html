<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Siyang Qin</title>
  <meta name="author" content="Siyang Qin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/me.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Siyang Qin</name>
              </p>
              <p>
                I am a senior software engineer at <a href="https://research.google/teams/perception/">Google Research, Perception</a>. My research interest lies at the intersection of computer vision and machine learning, with the focus on optical character recognition (OCR).
              </p>

              </p>
                Previously, I received my B.E. degree from <a href="https://www.tsinghua.edu.cn/en/" target="_blank">Tsinghua University</a> and Ph.D. degree from <a href="https://engineering.ucsc.edu/" target="_blank">University of California, Santa Cruz</a>, advised by <a href="https://users.soe.ucsc.edu/~manduchi/" target="_blank">Professor Roberto Manduchi</a>.
              </p>
              <p style="text-align:center">
                qinb[at]google.com &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ue_C8rMAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/siyang-qin-12b98467" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/qinbbbb" target="_blank"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img src="images/me.jpg" width="180" style="border-radius:50%" class="profile-image">
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2022: The <a href="https://github.com/google-research-datasets/hiertext", target="_blank">HierText dataset</a> is publicly available now.</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2022: Our paper on <a href="https://arxiv.org/abs/2203.15143", target="_blank">Towards End-to-End Unified Scene Text Detection and Layout Analysis</a> is accepted to CVPR 2022.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cvpr2022.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Towards End-to-End Unified Scene Text Detection and Layout Analysis</papertitle>
              <br>
              Shangbang Long, <strong>Siyang Qin</strong>, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, Michalis Raptis
              <br>
              CVPR 2022
              <br>
              <a href="https://arxiv.org/abs/2203.15143", target="_blank">arXiv</a> /
              <a href="https://github.com/google-research-datasets/hiertext", target="_blank">github</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acl2021.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction
</papertitle>
              <br>
              Chen-Yu Lee, Chun-Liang Li, Chu Wang, Renshen Wang, Yasuhisa Fujii, <strong>Siyang Qin</strong>, Ashok Popat, Tomas Pfister
              <br>
              ACL 2021 (<font color="red">Oral Presentation</font>)
              <br>
              <a href="https://arxiv.org/abs/2106.10786", target="_blank">arXiv</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tlrecog.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Rethinking Text Line Recognition Models</papertitle>
              <br>
              Daniel Hernandez Diaz, <strong>Siyang Qin</strong>, Reeve Ingle, Yasuhisa Fujii, Alessandro Bissacco
              <a href="https://arxiv.org/abs/2104.07787", target="_blank">arXiv</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv2019.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Towards Unconstrained End-to-End Text Spotting</papertitle>
              <br>
              <strong>Siyang Qin</strong>, Alessandro Bissacco, Michalis Raptis, Yasuhisa Fujii, Ying Xiao
              <br>
              ICCV 2019 (<font color="red">Oral Presentation</font>)
              <br>
              <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_Towards_Unconstrained_End-to-End_Text_Spotting_ICCV_2019_paper.pdf", target="_blank">paper</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iui2019.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Scene Text Access: A Comparison of Mobile OCR Modalities for Blind Users</papertitle>
              <br>
              Leo Neat, Ren Peng, <strong>Siyang Qin</strong>, Roberto Manduchi
              <br>
              IUI 2019
              <br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3301275.3302271", target="_blank">paper</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bmvc2018.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Automatic Semantic Content Removal by Learning to Neglect</papertitle>
              <br>
              <strong>Siyang Qin</strong>, Jiahui Wei, Roberto Manduchi
              <br>
              BMVC 2018 (<font color="red">Best Industry Paper Award</font>)
              <br>
              <a href="https://arxiv.org/pdf/1807.07696.pdf", target="_blank">arXiv</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/3dv2018.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Multi-planar Monocular Reconstruction of Manhattan Indoor Scenes</papertitle>
              <br>
              Seongdo Kim, Roberto Manduchi, <strong>Siyang Qin</strong>
              <br>
              3DV 2018
              <br>
              <a href="files/3DV2018.pdf", target="_blank">paper</a>
            </td>
          </tr>
           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wacv2018.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Robust and Accurate Text Stroke Segmentation</papertitle>
              <br>
              <strong>Siyang Qin</strong>, Peng Ren, Seongdo Kim, Roberto Manduchi
              <br>
              WACV 2018
              <br>
              <a href="https://escholarship.org/content/qt4pz3r5t3/qt4pz3r5t3.pdf", target="_blank">paper</a>
            </td>
          </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icdar2017.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Cascaded Segmentation-Detection Networks for Word-Level Text Spotting</papertitle>
              <br>
              <strong>Siyang Qin</strong>, Roberto Manduchi
              <br>
              ICDAR 2017
              <br>
              <a href="files/ICDAR2017.pdf", target="_blank">paper</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icme2017.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Automatic Skin and Hair Masking using Fully Convolutional Networks</papertitle>
              <br>
              <strong>Siyang Qin</strong>, Seongdo Kim, Roberto Manduchi
              <br>
              ICME 2017 (<font color="red">Oral Presentation</font>)
              <br>
              <a href="files/ICME2017.pdf", target="_blank">paper</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/wacv2016.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Fast and Robust Text Spotter</papertitle>
              <br>
              <strong>Siyang Qin</strong>, Roberto Manduchi
              <br>
              WACV 2016
              <br>
              <a href="files/WACV2016.pdf", target="_blank">paper</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/spie2015.png" alt="PontTuset" width="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Dynamic Mapping for Multiview Autostereoscopic Displays</papertitle>
              <br>
              Jing Liu, Tom Malzbender, <strong>Siyang Qin</strong>, Bipeng Zhang, Che-AnWu, James Davis
              <br>
              IS&T/SPIE Electronic Imaging, 2015
              <br>
              <a href="files/SPIE2015.pdf", target="_blank">paper</a>
            </td>
          </tr>
        </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Source code credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a></p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
